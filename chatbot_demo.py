"""
ğŸ¤– VieNeu-TTS: Chatbot Demo vá»›i LLM + TTS Streaming
MÃ´ phá»ng há»‡ thá»‘ng chatbot thá»i gian thá»±c vá»›i latency tháº¥p

Pipeline:
[User Input] â†’ [LLM streaming] â†’ [TTS streaming] â†’ [Audio Output]
"""

from vieneu import Vieneu
import numpy as np
import soundfile as sf
import os
import time
from typing import Generator

# ============================================================================
# Simulated LLM Streaming (Giáº£ láº­p LLM response)
# Thay tháº¿ báº±ng OpenAI, Ollama, hoáº·c LLM thá»±c cá»§a báº¡n
# ============================================================================

def simulate_llm_stream(user_message: str) -> Generator[str, None, None]:
    """
    Giáº£ láº­p LLM streaming response.
    Trong thá»±c táº¿, thay tháº¿ báº±ng:
    - OpenAI: openai.ChatCompletion.create(stream=True)
    - Ollama: ollama.chat(stream=True)
    - Local LLM: llama-cpp-python vá»›i stream=True
    """
    # Simulated response based on input
    responses = {
        "xin chÃ o": "Xin chÃ o báº¡n! TÃ´i lÃ  trá»£ lÃ½ áº£o VieNeu. TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n hÃ´m nay?",
        "thá»i tiáº¿t": "HÃ´m nay thá»i tiáº¿t khÃ¡ Ä‘áº¹p, trá»i náº¯ng nháº¹ vÃ  nhiá»‡t Ä‘á»™ khoáº£ng 25 Ä‘á»™ C. Ráº¥t thÃ­ch há»£p Ä‘á»ƒ ra ngoÃ i dáº¡o chÆ¡i.",
        "giá»›i thiá»‡u": "TÃ´i lÃ  VieNeu, má»™t há»‡ thá»‘ng chuyá»ƒn vÄƒn báº£n thÃ nh giá»ng nÃ³i tiáº¿ng Viá»‡t. TÃ´i cÃ³ thá»ƒ Ä‘á»c vÄƒn báº£n, clone giá»ng nÃ³i, vÃ  há»— trá»£ chatbot thá»i gian thá»±c.",
    }
    
    # Default response
    response = responses.get(
        user_message.lower().strip(),
        "Cáº£m Æ¡n báº¡n Ä‘Ã£ nháº¯n tin. ÄÃ¢y lÃ  pháº£n há»“i máº«u tá»« chatbot. Trong thá»±c táº¿, báº¡n cÃ³ thá»ƒ káº¿t ná»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° GPT hoáº·c Gemini."
    )
    
    # Simulate streaming: yield tá»«ng tá»« vá»›i delay nhá»
    words = response.split()
    buffer = ""
    
    for i, word in enumerate(words):
        buffer += word + " "
        time.sleep(0.05)  # Simulate LLM token generation delay (50ms/token)
        
        # Yield khi Ä‘á»§ cÃ¢u hoáº·c Ä‘á»§ dÃ i
        if word.endswith(('.', '!', '?', ',')) or len(buffer) > 50:
            yield buffer.strip()
            buffer = ""
    
    # Yield remaining
    if buffer.strip():
        yield buffer.strip()


# ============================================================================
# Real LLM Integration Examples (Uncomment to use)
# ============================================================================

# def openai_llm_stream(user_message: str) -> Generator[str, None, None]:
#     """Stream tá»« OpenAI GPT"""
#     import openai
#     client = openai.OpenAI(api_key="your-api-key")
#     
#     stream = client.chat.completions.create(
#         model="gpt-4o-mini",
#         messages=[{"role": "user", "content": user_message}],
#         stream=True
#     )
#     
#     buffer = ""
#     for chunk in stream:
#         if chunk.choices[0].delta.content:
#             buffer += chunk.choices[0].delta.content
#             # Yield khi gáº·p dáº¥u cÃ¢u
#             if buffer.rstrip().endswith(('.', '!', '?', ',')):
#                 yield buffer.strip()
#                 buffer = ""
#     if buffer.strip():
#         yield buffer.strip()


# def ollama_llm_stream(user_message: str) -> Generator[str, None, None]:
#     """Stream tá»« Ollama local LLM"""
#     import ollama
#     
#     stream = ollama.chat(
#         model='llama3.2',
#         messages=[{"role": "user", "content": user_message}],
#         stream=True
#     )
#     
#     buffer = ""
#     for chunk in stream:
#         buffer += chunk['message']['content']
#         if buffer.rstrip().endswith(('.', '!', '?', ',')):
#             yield buffer.strip()
#             buffer = ""
#     if buffer.strip():
#         yield buffer.strip()


# ============================================================================
# Main Chatbot Pipeline
# ============================================================================

def chatbot_pipeline(user_message: str, tts: Vieneu, voice_data: dict):
    """
    Pipeline chatbot hoÃ n chá»‰nh:
    1. Nháº­n tin nháº¯n user
    2. Stream response tá»« LLM
    3. Stream TTS cho tá»«ng pháº§n response
    4. PhÃ¡t audio real-time
    """
    print(f"\nğŸ‘¤ User: {user_message}")
    print("=" * 60)
    
    pipeline_start = time.perf_counter()
    first_audio_time = None
    
    all_audio_chunks = []
    all_text_parts = []
    
    print("\nğŸ¤– Assistant (streaming):")
    print("-" * 60)
    
    # Stream tá»« LLM vÃ  TTS song song
    for text_chunk in simulate_llm_stream(user_message):
        chunk_start = time.perf_counter()
        all_text_parts.append(text_chunk)
        
        print(f"  ğŸ’¬ LLM: \"{text_chunk}\"")
        
        # Stream TTS cho Ä‘oáº¡n text nÃ y
        for audio_chunk in tts.infer_stream(
            text=text_chunk,
            voice=voice_data,
            max_chars=256,
            temperature=1.0,
            top_k=50
        ):
            if first_audio_time is None:
                first_audio_time = time.perf_counter() - pipeline_start
                print(f"\n  ğŸš€ FIRST AUDIO LATENCY: {first_audio_time*1000:.0f}ms")
                print("-" * 60)
            
            all_audio_chunks.append(audio_chunk)
            audio_ms = len(audio_chunk) / 24000 * 1000
            print(f"  ğŸ”Š Audio chunk: {audio_ms:.0f}ms")
    
    # Tá»•ng há»£p káº¿t quáº£
    pipeline_end = time.perf_counter()
    total_time = pipeline_end - pipeline_start
    
    if all_audio_chunks:
        final_audio = np.concatenate(all_audio_chunks)
        audio_duration = len(final_audio) / 24000
        rtf = total_time / audio_duration
    else:
        final_audio = np.array([])
        audio_duration = 0
        rtf = 0
    
    print("\n" + "=" * 60)
    print("ğŸ“Š CHATBOT PIPELINE LATENCY REPORT")
    print("=" * 60)
    print(f"  ğŸ“ˆ First Audio Latency:    {first_audio_time*1000:.0f}ms" if first_audio_time else "  ğŸ“ˆ First Audio Latency:    N/A")
    print(f"  ğŸ“ˆ Total Pipeline Time:    {total_time*1000:.0f}ms ({total_time:.2f}s)")
    print(f"  ğŸ“ˆ Audio Duration:         {audio_duration*1000:.0f}ms ({audio_duration:.2f}s)")
    print(f"  ğŸ“ˆ Real-Time Factor:       {rtf:.2f}x")
    print(f"  ğŸ“ˆ LLM Chunks:             {len(all_text_parts)}")
    print(f"  ğŸ“ˆ Audio Chunks:           {len(all_audio_chunks)}")
    print("=" * 60)
    
    return final_audio, " ".join(all_text_parts)


def main():
    print("ğŸ¤– VieNeu Chatbot Demo - LLM + TTS Streaming")
    print("=" * 60)
    
    os.makedirs("outputs", exist_ok=True)
    
    # Khá»Ÿi táº¡o TTS
    print("\nâ³ Äang khá»Ÿi táº¡o TTS engine...")
    init_start = time.perf_counter()
    
    tts = Vieneu(
        backbone_repo="pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf",
        backbone_device="cpu",
        codec_repo="neuphonic/distill-neucodec",
        codec_device="cpu"
    )
    
    init_time = time.perf_counter() - init_start
    print(f"âœ… TTS khá»Ÿi táº¡o trong {init_time:.2f}s")
    
    voice_data = tts.get_preset_voice()
    
    # Demo cÃ¡c cÃ¢u há»i
    test_messages = [
        "xin chÃ o",
        "giá»›i thiá»‡u",
        "thá»i tiáº¿t"
    ]
    
    for i, message in enumerate(test_messages, 1):
        print(f"\n{'#' * 60}")
        print(f"# TEST {i}/{len(test_messages)}")
        print(f"{'#' * 60}")
        
        audio, response_text = chatbot_pipeline(message, tts, voice_data)
        
        if len(audio) > 0:
            output_path = f"outputs/chatbot_response_{i}.wav"
            sf.write(output_path, audio, 24000)
            print(f"\nğŸ’¾ ÄÃ£ lÆ°u: {output_path}")
    
    tts.close()
    print("\nğŸ‰ Demo hoÃ n thÃ nh!")


if __name__ == "__main__":
    main()
