"""
üé§ VieNeu-TTS: ASR + LLM + TTS Demo cho Google Colab
Pipeline: [Mic] ‚Üí [ASR] ‚Üí [LLM streaming] ‚Üí [TTS streaming] ‚Üí [Audio Output]

S·ª≠ d·ª•ng l·∫°i c√°c h√†m t·ª´ llm_tts_demo.py m√† kh√¥ng s·ª≠a ƒë·ªïi.

C√†i ƒë·∫∑t tr√™n Colab:
    !pip install faster-whisper google-genai python-dotenv
"""

import time
import os
import numpy as np
import base64
import io
from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# Import t·ª´ llm_tts_demo.py (KH√îNG s·ª≠a file g·ªëc)
# ============================================================================
from llm_tts_demo import gemini_stream, ColabPlayer

# ============================================================================
# JavaScript ghi √¢m t·ª´ Microphone tr√™n Colab
# ============================================================================

RECORD_JS = """
async function recordAudio(durationMs) {
    // Xin quy·ªÅn truy c·∫≠p microphone
    const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true
        }
    });
    
    const mediaRecorder = new MediaRecorder(stream, {mimeType: 'audio/webm'});
    const chunks = [];
    
    mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) chunks.push(e.data);
    };
    
    // B·∫Øt ƒë·∫ßu ghi
    mediaRecorder.start();
    
    // Hi·ªÉn th·ªã ƒë·∫øm ng∆∞·ª£c
    const startTime = Date.now();
    const updateInterval = setInterval(() => {
        const elapsed = ((Date.now() - startTime) / 1000).toFixed(1);
        document.getElementById('record-status').innerText = 
            'üî¥ ƒêang ghi √¢m... ' + elapsed + 's';
    }, 100);
    
    // ƒê·ª£i ƒë·ªß th·ªùi gian ho·∫∑c user nh·∫•n stop
    await new Promise(resolve => {
        // Auto-stop sau durationMs
        setTimeout(() => {
            resolve();
        }, durationMs);
        
        // Ho·∫∑c user nh·∫•n n√∫t stop
        window._stopRecording = resolve;
    });
    
    clearInterval(updateInterval);
    mediaRecorder.stop();
    stream.getTracks().forEach(track => track.stop());
    
    // ƒê·ª£i MediaRecorder x·ª≠ l√Ω xong
    await new Promise(resolve => {
        mediaRecorder.onstop = resolve;
    });
    
    // Convert sang base64
    const blob = new Blob(chunks, {type: 'audio/webm'});
    const reader = new FileReader();
    const base64Promise = new Promise(resolve => {
        reader.onloadend = () => resolve(reader.result.split(',')[1]);
    });
    reader.readAsDataURL(blob);
    
    return await base64Promise;
}
"""

STOP_BUTTON_JS = """
(function() {
    if (window._stopRecording) {
        window._stopRecording();
    }
})();
"""


def record_audio_colab(duration_seconds=10):
    """
    Ghi √¢m t·ª´ microphone tr√™n Google Colab.
    
    Args:
        duration_seconds: Th·ªùi gian ghi t·ªëi ƒëa (gi√¢y)
    
    Returns:
        numpy array ch·ª©a audio (float32)
    """
    from google.colab import output
    from IPython.display import display, HTML, Javascript
    
    # T·∫°o UI
    display(HTML("""
        <div style="padding: 10px; background: #1a1a2e; border-radius: 8px; 
                    color: white; font-family: monospace;">
            <p id="record-status" style="font-size: 16px;">
                ‚è≥ Chu·∫©n b·ªã ghi √¢m...
            </p>
            <button onclick="window._stopRecording && window._stopRecording()" 
                    style="padding: 8px 20px; background: #e94560; color: white; 
                           border: none; border-radius: 4px; cursor: pointer;
                           font-size: 14px; margin-top: 5px;">
                ‚èπ D·ª´ng ghi √¢m
            </button>
        </div>
    """))
    
    # Inject JS v√† ghi √¢m
    duration_ms = duration_seconds * 1000
    audio_base64 = output.eval_js(RECORD_JS + f"\nrecordAudio({duration_ms})")
    
    if not audio_base64:
        print("‚ùå Kh√¥ng ghi ƒë∆∞·ª£c √¢m thanh!")
        return None
    
    # Decode base64 ‚Üí audio bytes
    audio_bytes = base64.b64decode(audio_base64)
    
    # L∆∞u t·∫°m th√†nh file webm r·ªìi d√πng ffmpeg/pydub convert
    tmp_webm = "/tmp/colab_recording.webm"
    tmp_wav = "/tmp/colab_recording.wav"
    
    with open(tmp_webm, "wb") as f:
        f.write(audio_bytes)
    
    # Convert webm ‚Üí wav 16kHz mono b·∫±ng ffmpeg (c√≥ s·∫µn tr√™n Colab)
    os.system(f"ffmpeg -y -i {tmp_webm} -ar 16000 -ac 1 {tmp_wav} -loglevel quiet")
    
    # ƒê·ªçc WAV file
    import soundfile as sf
    audio_array, sr = sf.read(tmp_wav, dtype='float32')
    
    duration = len(audio_array) / sr
    print(f"‚úÖ Ghi √¢m xong: {duration:.1f}s ({len(audio_array)} samples, {sr}Hz)")
    
    return audio_array


def record_audio_local(duration_seconds=5, sample_rate=16000):
    """
    Ghi √¢m t·ª´ microphone tr√™n local (d√πng sounddevice).
    Fallback khi kh√¥ng ch·∫°y tr√™n Colab.
    """
    import sounddevice as sd
    
    print(f"üé§ ƒêang ghi √¢m {duration_seconds}s... (n√≥i ti·∫øng Vi·ªát)")
    audio = sd.rec(
        int(duration_seconds * sample_rate),
        samplerate=sample_rate,
        channels=1,
        dtype='float32'
    )
    sd.wait()
    print("‚úÖ Ghi √¢m xong!")
    
    return audio.flatten()


# ============================================================================
# ASR - Nh·∫≠n d·∫°ng gi·ªçng n√≥i
# ============================================================================

_asr_model = None

def load_asr_model(model_size="base", device="cuda", compute_type="float16"):
    """Load Faster-Whisper model (ch·ªâ load 1 l·∫ßn)."""
    global _asr_model
    if _asr_model is None:
        from faster_whisper import WhisperModel
        print(f"‚è≥ ƒêang t·∫£i ASR model (faster-whisper {model_size})...")
        try:
            _asr_model = WhisperModel(model_size, device=device, compute_type=compute_type)
        except Exception:
            # Fallback: CPU v·ªõi int8
            print("‚ö†Ô∏è GPU kh√¥ng kh·∫£ d·ª•ng cho ASR, d√πng CPU...")
            _asr_model = WhisperModel(model_size, device="cpu", compute_type="int8")
        print("‚úÖ ASR model ƒë√£ s·∫µn s√†ng!")
    return _asr_model


def transcribe(audio_array, model_size="base"):
    """
    Chuy·ªÉn audio ‚Üí text b·∫±ng Faster-Whisper.
    
    Args:
        audio_array: numpy array (float32, 16kHz)
        model_size: k√≠ch th∆∞·ªõc model Whisper
    
    Returns:
        text ƒë√£ nh·∫≠n d·∫°ng
    """
    model = load_asr_model(model_size)
    
    segments, info = model.transcribe(
        audio_array,
        language="vi",
        beam_size=5,
        vad_filter=True  # L·ªçc kho·∫£ng im l·∫∑ng
    )
    
    text = " ".join([segment.text for segment in segments]).strip()
    return text


# ============================================================================
# Pipeline ASR ‚Üí LLM ‚Üí TTS v·ªõi ƒëo latency
# ============================================================================

def asr_llm_tts_pipeline(tts, voice, api_key, 
                          asr_model_size="base",
                          record_duration=10,
                          gemini_model="gemini-2.0-flash"):
    """
    Pipeline ƒë·∫ßy ƒë·ªß: Mic ‚Üí ASR ‚Üí LLM ‚Üí TTS
    
    ƒêo c√°c m·ªëc latency:
    - ASR Latency: th·ªùi gian nh·∫≠n d·∫°ng gi·ªçng n√≥i
    - LLM First Token: th·ªùi gian ch·ªù LLM token ƒë·∫ßu ti√™n
    - TTFC: LLM text ƒë·∫ßu ti√™n ‚Üí Audio ƒë·∫ßu ti√™n 
    - E2E Latency: t·ª´ nh·∫•n Enter (k·∫øt th√∫c ghi √¢m) ‚Üí Audio ƒë·∫ßu ti√™n
    """
    from IPython.display import Audio, display
    
    # === B∆Ø·ªöC 0: Ghi √¢m ===
    print("\n" + "=" * 60)
    print("üé§ B∆Ø·ªöC 1: GHI √ÇM")
    print("=" * 60)
    
    audio_input = record_audio_colab(duration_seconds=record_duration)
    if audio_input is None or len(audio_input) == 0:
        print("‚ùå Kh√¥ng c√≥ √¢m thanh, th·ª≠ l·∫°i!")
        return None
    
    # ====================================================
    # t_start: M·ªëc b·∫Øt ƒë·∫ßu ƒëo E2E (ngay sau khi ghi √¢m xong / nh·∫•n Enter)
    # ====================================================
    t_start = time.perf_counter()
    
    # === B∆Ø·ªöC 1: ASR ===
    print("\n" + "=" * 60)
    print("üìù B∆Ø·ªöC 2: NH·∫¨N D·∫†NG GI·ªåNG N√ìI (ASR)")
    print("=" * 60)
    
    recognized_text = transcribe(audio_input, model_size=asr_model_size)
    t_asr_done = time.perf_counter()
    asr_latency = t_asr_done - t_start
    
    print(f"üìù ASR k·∫øt qu·∫£: \"{recognized_text}\"")
    print(f"‚è±Ô∏è  ASR Latency: {asr_latency*1000:.0f}ms")
    
    if not recognized_text:
        print("‚ùå Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c gi·ªçng n√≥i!")
        return None
    
    # === B∆Ø·ªöC 2: LLM + TTS Streaming ===
    print("\n" + "=" * 60)
    print("ü§ñ B∆Ø·ªöC 3: LLM + TTS STREAMING")
    print("=" * 60)
    
    llm_fn = lambda msg: gemini_stream(msg, api_key, gemini_model)
    
    # S·ª≠ d·ª•ng ColabPlayer t·ª´ llm_tts_demo.py
    player = ColabPlayer(sample_rate=24000)
    player.start()
    
    all_audio = []
    all_texts = []
    t_first_llm = None
    t_first_audio = None
    chunk_count = 0
    
    for text_chunk in llm_fn(recognized_text):
        # ƒêo LLM first token
        if t_first_llm is None:
            t_first_llm = time.perf_counter()
            llm_first_token_latency = t_first_llm - t_asr_done
            print(f"\nüí¨ LLM first token: {llm_first_token_latency*1000:.0f}ms (sau ASR)")
        
        all_texts.append(text_chunk)
        print(f"  üí¨ LLM: \"{text_chunk}\"")
        
        # TTS streaming
        for audio_chunk in tts.infer_stream(
            text=text_chunk,
            voice=voice,
            max_chars=256,
            temperature=1.0,
            top_k=50
        ):
            chunk_count += 1
            
            # ƒêo first audio
            if t_first_audio is None:
                t_first_audio = time.perf_counter()
                ttfc = t_first_audio - t_first_llm
                e2e_latency = t_first_audio - t_start
                print(f"\n  üöÄ FIRST AUDIO!")
                print(f"     TTFC (LLM‚ÜíAudio): {ttfc*1000:.0f}ms")
                print(f"     E2E  (Enter‚ÜíAudio): {e2e_latency*1000:.0f}ms")
            
            player.add_chunk(audio_chunk)
            all_audio.append(audio_chunk)
            
            audio_ms = len(audio_chunk) / 24000 * 1000
            print(f"  üîä Audio chunk {chunk_count}: {audio_ms:.0f}ms")
    
    # Ph√°t audio
    player.stop()
    
    t_end = time.perf_counter()
    total_time = t_end - t_start
    
    # === B√ÅO C√ÅO LATENCY ===
    print("\n" + "=" * 60)
    print("üìä LATENCY REPORT: ASR ‚Üí LLM ‚Üí TTS")
    print("=" * 60)
    
    full_text = " ".join(all_texts)
    full_audio = np.concatenate(all_audio) if all_audio else np.array([])
    audio_duration = len(full_audio) / 24000 if len(full_audio) > 0 else 0
    
    print(f"  üé§ Input:               \"{recognized_text}\"")
    print(f"  ü§ñ Response:            \"{full_text[:100]}{'...' if len(full_text) > 100 else ''}\"")
    print(f"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print(f"  ‚è±Ô∏è  ASR Latency:         {asr_latency*1000:.0f}ms")
    
    if t_first_llm:
        print(f"  ‚è±Ô∏è  LLM First Token:     {llm_first_token_latency*1000:.0f}ms")
    
    if t_first_audio:
        print(f"  ‚è±Ô∏è  TTFC (LLM‚ÜíAudio):    {ttfc*1000:.0f}ms")
        print(f"  ‚è±Ô∏è  E2E  (Enter‚ÜíAudio):  {e2e_latency*1000:.0f}ms  ‚Üê ƒê·ªò TR·ªÑ T·ªîNG")
    
    print(f"  ‚è±Ô∏è  Total Pipeline:      {total_time*1000:.0f}ms")
    print(f"  üîä Audio Duration:       {audio_duration:.2f}s")
    print(f"  üìä Audio Chunks:         {chunk_count}")
    
    if audio_duration > 0:
        rtf = total_time / audio_duration
        print(f"  üìä Real-Time Factor:     {rtf:.2f}x")
    
    print("=" * 60)
    
    return {
        "recognized_text": recognized_text,
        "response_text": full_text,
        "audio": full_audio,
        "asr_latency_ms": asr_latency * 1000,
        "llm_first_token_ms": llm_first_token_latency * 1000 if t_first_llm else None,
        "ttfc_ms": ttfc * 1000 if t_first_audio else None,
        "e2e_latency_ms": e2e_latency * 1000 if t_first_audio else None,
        "total_time_ms": total_time * 1000,
    }


# ============================================================================
# Main - ch·∫°y tr√™n Colab
# ============================================================================

def main():
    """
    Main function - ch·∫°y tr√™n Google Colab.
    
    C√°ch d√πng trong Colab notebook:
        from colab_asr_demo import main
        main()
    
    Ho·∫∑c ch·∫°y t·ª´ng b∆∞·ªõc:
        from colab_asr_demo import load_asr_model, asr_llm_tts_pipeline
        from vieneu import Vieneu
    """
    from IPython.display import display, HTML
    
    print("üé§ü§ñüîä VieNeu: Voice-to-Voice Demo")
    print("Pipeline: Mic ‚Üí ASR ‚Üí LLM ‚Üí TTS")
    print("=" * 60)
    
    # --- C·∫•u h√¨nh ---
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        try:
            from google.colab import userdata
            api_key = userdata.get("GEMINI_API_KEY")
        except Exception:
            pass
    if not api_key:
        api_key = input("Nh·∫≠p Gemini API Key: ").strip()
    
    # --- Load models ---
    print("\n‚è≥ ƒêang t·∫£i ASR model...")
    load_asr_model(model_size="base")
    
    print("\n‚è≥ ƒêang t·∫£i TTS engine...")
    from vieneu import Vieneu
    
    # Auto-detect GPU
    try:
        import torch
        has_cuda = torch.cuda.is_available()
    except ImportError:
        has_cuda = False
    
    if has_cuda:
        tts = Vieneu(
            backbone_repo="pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf",
            backbone_device="gpu",
            codec_repo="neuphonic/distill-neucodec",
            codec_device="cuda"
        )
        print("üöÄ GPU mode!")
    else:
        tts = Vieneu(
            backbone_repo="pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf",
            backbone_device="cpu",
            codec_repo="neuphonic/distill-neucodec",
            codec_device="cpu"
        )
        print("üíª CPU mode")
    
    voice = tts.get_preset_voice()
    print("‚úÖ T·∫•t c·∫£ models ƒë√£ s·∫µn s√†ng!\n")
    
    # --- V√≤ng l·∫∑p h·ªôi tho·∫°i ---
    round_num = 0
    while True:
        round_num += 1
        print(f"\n{'#' * 60}")
        print(f"# L∆Ø·ª¢T {round_num}")
        print(f"{'#' * 60}")
        
        cont = input("\nüé§ Nh·∫•n Enter ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m (ho·∫∑c g√µ 'exit'): ").strip()
        if cont.lower() == 'exit':
            break
        
        result = asr_llm_tts_pipeline(
            tts=tts,
            voice=voice,
            api_key=api_key,
            asr_model_size="base",
            record_duration=10,
            gemini_model="gemini-2.0-flash"
        )
        
        if result:
            print(f"\n‚úÖ L∆∞·ª£t {round_num} ho√†n th√†nh!")
    
    tts.close()
    print("\nüëã K·∫øt th√∫c demo!")


if __name__ == "__main__":
    main()
