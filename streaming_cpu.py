"""
VieNeu-TTS: CPU Streaming Demo with Latency Measurement
Äo latency khi sinh Ã¢m thanh streaming trÃªn CPU
"""

from vieneu import Vieneu
import numpy as np
import soundfile as sf
import os
import time

def main():
    print("Khá»Ÿi táº¡o VieNeu-TTS cho CPU streaming")
    print("=" * 60)
    
    os.makedirs("outputs", exist_ok=True)
    
    init_start = time.perf_counter()
    
    tts = Vieneu(
        backbone_repo="pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf",
        backbone_device="cpu",
        codec_repo="neuphonic/distill-neucodec",
        codec_device="cpu"
    )
    
    init_time = time.perf_counter() - init_start
    print(f"\n Thá»i gian khá»Ÿi táº¡o model: {init_time:.2f}s")
    print("=" * 60)
    
    # Text cáº§n Ä‘á»c
    text = """
    Xin chÃ o, Ä‘Ã¢y lÃ  demo TTS streaming cháº¡y hoÃ n toÃ n trÃªn CPU.
    Cháº¿ Ä‘á»™ streaming cho phÃ©p phÃ¡t Ã¢m thanh ngay khi Ä‘ang sinh.
    KhÃ´ng cáº§n Ä‘á»£i toÃ n bá»™ vÄƒn báº£n Ä‘Æ°á»£c xá»­ lÃ½ xong.
    """
    
    # Láº¥y voice máº·c Ä‘á»‹nh
    voice_data = tts.get_preset_voice()
    print(f"ğŸ¤ Sá»­ dá»¥ng voice máº·c Ä‘á»‹nh")
    print(f"Text: {len(text.strip())} kÃ½ tá»±")
    print("=" * 60)
    
    print("\n Báº¯t Ä‘áº§u streaming inference...")
    print("-" * 60)
    
    # ===== Äo latency cho tá»«ng chunk =====
    audio_chunks = []
    chunk_latencies = []
    chunk_count = 0
    
    # Thá»i Ä‘iá»ƒm báº¯t Ä‘áº§u inference
    inference_start = time.perf_counter()
    first_chunk_time = None
    
    for audio_chunk in tts.infer_stream(
        text=text,
        voice=voice_data,
        max_chars=256,
        temperature=1.0,
        top_k=50
    ):
        chunk_end = time.perf_counter()
        chunk_count += 1
        
        # Äo Time-to-First-Chunk (TTFC)
        if first_chunk_time is None:
            first_chunk_time = chunk_end - inference_start
            print(f"\n  TIME-TO-FIRST-CHUNK (TTFC): {first_chunk_time*1000:.0f}ms")
            print("-" * 60)
        
        audio_chunks.append(audio_chunk)
        
        # TÃ­nh latency vÃ  thÃ´ng tin chunk
        chunk_duration_ms = len(audio_chunk) / 24000 * 1000  # Audio duration
        elapsed = chunk_end - inference_start
        chunk_latencies.append(elapsed)
        
        print(f"  Chunk {chunk_count}:")
        print(f"     â€¢ Samples: {len(audio_chunk):,}")
        print(f"     â€¢ Audio duration: {chunk_duration_ms:.0f}ms")
        print(f"     â€¢ Elapsed time: {elapsed*1000:.0f}ms")
    
    # ===== Tá»•ng há»£p káº¿t quáº£ latency =====
    inference_end = time.perf_counter()
    total_inference_time = inference_end - inference_start
    
    print("-" * 60)
    print("\n    LATENCY REPORT")
    print("=" * 60)
    
    # GhÃ©p audio
    final_audio = np.concatenate(audio_chunks)
    total_audio_duration = len(final_audio) / 24000
    
    # TÃ­nh Real-Time Factor (RTF)
    rtf = total_inference_time / total_audio_duration
    
    print(f" Time-to-First-Chunk (TTFC): {first_chunk_time*1000:.0f}ms")
    print(f" Total inference time:       {total_inference_time*1000:.0f}ms ({total_inference_time:.2f}s)")
    print(f" Total audio duration:       {total_audio_duration*1000:.0f}ms ({total_audio_duration:.2f}s)")
    print(f" Real-Time Factor (RTF):     {rtf:.2f}x")
    print(f"     â””â”€ RTF < 1.0 = Faster than real-time ")
    print(f"     â””â”€ RTF > 1.0 = Slower than real-time ")
    print(f" Chunks generated:           {chunk_count}")
    print(f" Avg latency per chunk:      {(total_inference_time/chunk_count)*1000:.0f}ms")
    
    print("=" * 60)
    
    # LÆ°u file WAV
    output_path = "outputs/streaming_output.wav"
    sf.write(output_path, final_audio, 24000)
    print(f"\n ÄÃ£ lÆ°u: {output_path}")
    
    # Cleanup
    tts.close()
    print("Done!")

if __name__ == "__main__":
    main()
